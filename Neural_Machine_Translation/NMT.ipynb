{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80cf903",
   "metadata": {},
   "source": [
    "## Neural Machine Translation with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290c6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pickle import dump\n",
    "from numpy import array\n",
    "from unicodedata import normalize\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101a34a",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132923f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3cb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = 'deu.txt'\n",
    "\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f249fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi.\\tHallo!\\nHi.\\tGrüß Gott!\\nRun!\\tLauf!\\nWow!\\tPotzdonn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7851265",
   "metadata": {},
   "source": [
    "#### We must split the loaded text by line and then by phrase. \n",
    "\n",
    "Each line contains a single pair of phrases, first English and then German, separated by a tab character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e2a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afc17d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hallo!'],\n",
       " ['Hi.', 'Grüß Gott!'],\n",
       " ['Run!', 'Lauf!'],\n",
       " ['Wow!', 'Potzdonner!'],\n",
       " ['Wow!', 'Donnerwetter!']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = to_pairs(doc)\n",
    "\n",
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126af5b5",
   "metadata": {},
   "source": [
    "#### Clean each sentence:\n",
    "\n",
    "- Remove all non-printable characters.\n",
    "- Remove all punctuation characters.\n",
    "- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "- Normalize the case to lowercase.\n",
    "- Remove any remaining tokens that are not alphabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095ce7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff28ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'gru gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ['wow', 'potzdonner'],\n",
       "       ['wow', 'donnerwetter']], dtype='<U370')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs = clean_pairs(pairs)\n",
    "\n",
    "clean_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d874e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76844c",
   "metadata": {},
   "source": [
    "#### Save the list of phrase pairs to a file.\n",
    "\n",
    "use the pickle API to save the list of clean text to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4942441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c233ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "# save clean pairs to file\n",
    "\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a7dc1",
   "metadata": {},
   "source": [
    "### 2. Split Text\n",
    "- either load the saved pickle or continue with the cleaned data saved in variable `clean_pairs`\n",
    "- We will simplify the problem by reducing the dataset to the first 10,000 examples in the file; these will be the shortest phrases in the dataset.\n",
    "\n",
    "- Further, we will then stake the first 9,000 of those as examples for training and the remaining 1,000 examples to test the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1afe747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6811a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e91437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'gru gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ['wow', 'potzdonner'],\n",
       "       ['wow', 'donnerwetter'],\n",
       "       ['fire', 'feuer'],\n",
       "       ['help', 'hilfe'],\n",
       "       ['help', 'zu hulf'],\n",
       "       ['stop', 'stopp'],\n",
       "       ['wait', 'warte']], dtype='<U370')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "raw_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3efa414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "\n",
    "dataset = raw_dataset[:n_sentences, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea5c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20eee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6e25f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30564.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20/100) * 152820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3877056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test [80-20]\n",
    "train, test = dataset[:122256], dataset[122256:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126782f3",
   "metadata": {},
   "source": [
    "#### Tokenize Text:\n",
    "    \n",
    "We will use separate tokenizer for the English sequences and the German sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a65a6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e561cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b447d",
   "metadata": {},
   "source": [
    "#### english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c2109e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dfe1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2404"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "316da183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the length of the longest sequence\n",
    "eng_length = max(len(line.split()) for line in dataset[:, 0])\n",
    "eng_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddaad01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2404\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d9eba",
   "metadata": {},
   "source": [
    "#### german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd27beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_tokenizer = create_tokenizer(dataset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9a107ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3856"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2c59c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the length of the longest sequence\n",
    "ger_length = max(len(line.split()) for line in dataset[:, 1])\n",
    "ger_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "328dc387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3856\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db1d26",
   "metadata": {},
   "source": [
    "#### Encode sequence to integers and pad to the maximum phrase length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41b36c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda6baf8",
   "metadata": {},
   "source": [
    "#### The output sequence needs to be one-hot encoded. This is because the model will predict the probability of each word in the vocabulary as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ade3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = tf.keras.utils.to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0a51514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3b34ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ca53e",
   "metadata": {},
   "source": [
    "### 3. Build Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4d1acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_vocab_size = 3856\n",
    "ger_length = 10\n",
    "\n",
    "eng_vocab_size = 2404\n",
    "eng_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c0575e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 256)           987136    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 2404)           617828    \n",
      "=================================================================\n",
      "Total params: 2,655,588\n",
      "Trainable params: 2,655,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(ger_vocab_size, 256, input_length=ger_length, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(256),\n",
    "    tf.keras.layers.RepeatVector(eng_length),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(eng_vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee040e64",
   "metadata": {},
   "source": [
    "#### Compile & Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77299560",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0201506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "157/157 [==============================] - 8s 23ms/step - loss: 5.3324\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 3.5441\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 3.3740\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 3.2190\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 3.0876\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 2.9445\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 2.7690\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 2.5909\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 2.3939\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 2.2229\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 2.0704\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.9451\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 13/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.8008\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 14/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 1.6744\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 15/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.5573\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 16/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.4461\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 17/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.3535\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 18/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.2468\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 19/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 1.1594\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 20/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 1.0676\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 21/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.9785\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 22/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.9046\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 23/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.8264\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 24/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.7624\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 25/50\n",
      "157/157 [==============================] - 3s 22ms/step - loss: 0.6939\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 26/50\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.6343\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 27/50\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 0.5795\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 28/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.5337\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 29/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.4841\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 30/50\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.4486\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 31/50\n",
      "157/157 [==============================] - 3s 22ms/step - loss: 0.4026\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 32/50\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 0.3700\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 33/50\n",
      "157/157 [==============================] - 3s 21ms/step - loss: 0.3559\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 34/50\n",
      "157/157 [==============================] - 3s 22ms/step - loss: 0.3189\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 35/50\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.3011\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 36/50\n",
      "157/157 [==============================] - 4s 25ms/step - loss: 0.2741\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 37/50\n",
      "157/157 [==============================] - 4s 23ms/step - loss: 0.2484\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 38/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.2353\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 39/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2235\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 40/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.2059\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 41/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1909\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 42/50\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.1811\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 43/50\n",
      "157/157 [==============================] - 3s 20ms/step - loss: 0.1691\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 44/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1618\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 45/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1576\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 46/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1493\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 47/50\n",
      "157/157 [==============================] - 3s 18ms/step - loss: 0.1424\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 48/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1328\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 49/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1280\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 50/50\n",
      "157/157 [==============================] - 3s 19ms/step - loss: 0.1264\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filename, monitor='val_loss', \n",
    "                                                verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "history = model.fit(trainX, trainY, epochs=50, batch_size=64, \n",
    "                    validation_data=(testX, testY), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea4d90",
   "metadata": {},
   "source": [
    "### 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc55bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5019d926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0e1ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74d03790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e67a87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f01ecd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s] ====> target=[%s] ====> predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "    # calculate BLEU score\n",
    "    print()\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd038087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate train data\n",
      "\n",
      "src=[sie mussen arbeiten] ====> target=[you must work] ====> predicted=[you must work]\n",
      "src=[das ist ihr wagen] ====> target=[that is her car] ====> predicted=[that is her car]\n",
      "src=[tom spendete beifall] ====> target=[tom applauded] ====> predicted=[tom applauded]\n",
      "src=[ich bin tvsuchtig] ====> target=[im a tv addict] ====> predicted=[im a tv addict]\n",
      "src=[haltet euch an die regeln] ====> target=[follow the rules] ====> predicted=[follow the rules]\n",
      "src=[das ist wichtig] ====> target=[it matters] ====> predicted=[its matters]\n",
      "src=[er wurde freigesprochen] ====> target=[he was acquitted] ====> predicted=[he was acquitted]\n",
      "src=[ich hatte gern eine limonade] ====> target=[id like a soda] ====> predicted=[id like a soda]\n",
      "src=[wir sind wohlhabend] ====> target=[were wealthy] ====> predicted=[were wealthy]\n",
      "src=[tom war traurig] ====> target=[tom felt sad] ====> predicted=[tom felt sad]\n",
      "\n",
      "BLEU-1: 0.950844\n",
      "BLEU-2: 0.934439\n",
      "BLEU-3: 0.878089\n",
      "BLEU-4: 0.642810\n"
     ]
    }
   ],
   "source": [
    "print('evaluate train data')\n",
    "print()\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ea7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dad0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
